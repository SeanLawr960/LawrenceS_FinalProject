{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b754381-b17f-4635-900c-736741c85e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/seanlawrence/Library/Python/3.11/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers gym deap numpy pandas scipy\n",
    "#%pip install nltk\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f12f4e-e39b-4a46-baf4-c45c75b2ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577d3a69-5b0e-47d3-bff0-16dcd057afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class TextGenerationEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, max_length=50):\n",
    "        super(TextGenerationEnv, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 50\n",
    "        self.action_space = spaces.Discrete(len(tokenizer))\n",
    "        self.observation_space = spaces.Box(0, len(tokenizer) - 1, (max_length,), dtype=np.int32)\n",
    "        self.last_gen_text = \"\"\n",
    "        self.threshold = 0.95\n",
    "        self.prompt=\"\"\n",
    "        self.top_k = 50\n",
    "        self.counter = 0\n",
    "\n",
    "    def reset(self, prompt=\"\"):\n",
    "        self.prompt = prompt\n",
    "        self.generated_text = [prompt]\n",
    "        self.current_length = len(self.generated_text)\n",
    "        self.counter = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        input_ids = self.tokenizer.encode(self.prompt, return_tensors='pt')\n",
    "        outputs = self.model.generate(input_ids, max_length=50, do_sample=True) \n",
    "\n",
    "        self.generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        reward = self._calculate_reward()\n",
    "        self.counter += 1\n",
    "        done = (reward == 1) or (self.counter > self.top_k)\n",
    "        #print(self.generated_text)\n",
    "        if done:\n",
    "            self.counter = 0\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array(self.tokenizer.encode(self.generated_text, max_length=self.max_length))\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        generated_text_str = ' '.join(self.generated_text)\n",
    "        if \"bank\" in self.generated_text or \"bank\" in self.generated_text:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.last_gen_text = generated_text_str\n",
    "       \n",
    "        return reward\n",
    "\n",
    "    def get_last_text(self):\n",
    "        return self.last_gen_text\n",
    "\n",
    "    def print_last_gen_test(self):\n",
    "        print( self.last_gen_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228f65f8-0f56-4754-b1b6-d2024151c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "INFO:root:Episode 1/20 completed with total reward: 1\n",
      "INFO:root:Episode 2/20 completed with total reward: 1\n",
      "INFO:root:Episode 3/20 completed with total reward: 1\n",
      "INFO:root:Episode 4/20 completed with total reward: 1\n",
      "INFO:root:Episode 5/20 completed with total reward: 1\n",
      "INFO:root:Episode 6/20 completed with total reward: 1\n",
      "INFO:root:Episode 7/20 completed with total reward: 1\n",
      "INFO:root:Episode 8/20 completed with total reward: 1\n",
      "INFO:root:Episode 9/20 completed with total reward: 1\n",
      "INFO:root:Episode 10/20 completed with total reward: 1\n",
      "INFO:root:Episode 11/20 completed with total reward: 1\n",
      "INFO:root:Episode 12/20 completed with total reward: 1\n",
      "INFO:root:Episode 13/20 completed with total reward: 1\n",
      "INFO:root:Episode 14/20 completed with total reward: 1\n",
      "INFO:root:Episode 15/20 completed with total reward: 1\n",
      "INFO:root:Episode 16/20 completed with total reward: 1\n",
      "INFO:root:Episode 17/20 completed with total reward: 1\n",
      "INFO:root:Episode 18/20 completed with total reward: 1\n",
      "INFO:root:Episode 19/20 completed with total reward: 1\n",
      "INFO:root:Episode 20/20 completed with total reward: 1\n",
      "INFO:root:Training completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Actions: [3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 3298, 11754, 1080, 326, 423, 4073, 5179, 286, 4138, 286, 3946, 284, 307, 2626, 290, 45922, 517, 284, 2666, 11, 422, 262, 3298, 3176, 5939, 284, 262, 1294, 7395, 290, 28486, 5939, 13, 383, 2274, 10731, 326, 36872, 262, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2575, 259, 326, 2957, 284, 262, 29921, 805, 17235, 13, 628, 198, 464, 1637, 1908, 284, 262, 1230, 706, 29921, 805, 14707, 290, 284, 663, 5096, 1814, 373, 22138, 1068, 832, 262, 523, 12, 7174, 29928, 13, 198, 198, 1, 1532, 257, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 49777, 7118, 286, 262, 3884, 14166, 832, 26610, 33672, 438, 4758, 318, 8998, 379, 1642, 262, 3773, 1663, 5443, 290, 517, 2952, 355, 6341, 423, 517, 1637, 287, 511, 16511, 13, 220, 1849, 464, 1103, 2071, 783, 318, 1771, 356, 423, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 3878, 39824, 13, 1114, 257, 890, 640, 6341, 290, 7713, 547, 7960, 546, 262, 1109, 326, 1992, 2486, 290, 13419, 4986, 22283, 2269, 342, 1008, 561, 407, 423, 262, 11917, 284, 4574, 832, 257, 1336, 14634, 2855, 319, 262, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1134, 349, 4914, 300, 929, 84, 11, 257, 1611, 286, 11447, 3331, 810, 262, 1230, 3544, 5293, 5153, 284, 9604, 262, 1181, 11, 663, 3259, 290, 1964, 7941, 13, 6363, 4870, 468, 645, 826, 284, 2071, 1637, 736, 284, 262, 1230, 2845, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2602, 44807, 198, 198, 1026, 318, 281, 2562, 1808, 326, 2476, 284, 307, 9373, 13, 198, 198, 6104, 996, 345, 815, 407, 423, 326, 1637, 11, 530, 815, 407, 307, 18786, 13158, 379, 717, 13, 198, 198, 464, 1637, 815, 407, 307, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 11377, 29928, 286, 36763, 33816, 17796, 287, 3648, 326, 262, 1230, 3066, 284, 1577, 503, 287, 1502, 284, 1104, 663, 898, 9992, 4560, 13, 198, 1870, 287, 3717, 618, 4486, 373, 991, 2111, 284, 787, 2565, 286, 2279, 11, 340, 373, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 6341, 13, 198, 198, 1026, 318, 257, 4065, 329, 257, 3331, 284, 651, 511, 2832, 319, 1637, 326, 714, 3538, 307, 973, 284, 2822, 5293, 5010, 393, 584, 5293, 7017, 13, 1374, 373, 326, 13351, 618, 477, 428, 373, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 3176, 12129, 960, 1169, 3176, 50025, 326, 857, 1597, 351, 1688, 6341, 26, 777, 3730, 508, 2251, 649, 3946, 11, 484, 821, 407, 1016, 284, 1037, 262, 1762, 1398, 13, 1119, 389, 1016, 284, 4423, 866, 262, 3773, 290, 1309, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 37, 268, 12, 43, 2306, 641, 3331, 10731, 11, 543, 468, 587, 281, 9812, 2728, 3879, 4679, 329, 867, 812, 13, 554, 262, 1613, 11, 3176, 28449, 547, 691, 530, 3038, 329, 661, 11, 262, 1849, 49, 5, 35, 40943, 1141, 3198, 286, 262, 3176, 10731, 318, 262, 220, 933, 12754, 1438, 784, 543, 714, 1612, 25, 366, 43621, 82, 13, 383, 1637, 318, 287, 262, 6341, 1911, 1406, 644, 338, 262, 1103, 1621, 994, 30, 198, 198, 5189, 14564, 11, 314, 1053, 6235, 503, 326, 428, 614, 338, 3176, 4902, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1134, 261, 462, 11, 543, 373, 2714, 416, 471, 13, 42, 13, 3176, 24161, 11, 262, 4452, 286, 262, 955, 44913, 286, 262, 20113, 2961, 428, 614, 960, 3003, 484, 1053, 18838, 9264, 286, 584, 471, 13, 50, 13, 3331, 6741, 13, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 428, 1080, 25, 703, 262, 6341, 290, 1263, 1597, 460, 18510, 262, 3015, 326, 318, 783, 852, 3350, 319, 262, 1263, 2428, 326, 2300, 11, 884, 355, 5704, 11, 13771, 11, 262, 1181, 286, 968, 1971, 11, 290, 508, 338, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1134, 305, 469, 12, 992, 7791, 287, 543, 262, 3331, 318, 2263, 287, 517, 1637, 621, 340, 1682, 2476, 284, 423, 287, 281, 11171, 12, 9526, 3331, 13, 198, 198, 2504, 318, 2391, 407, 257, 1917, 618, 262, 3482, 290, 262, 1334, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 47, 1173, 278, 220, 1849, 23317, 3883, 4081, 13, 921, 760, 11, 314, 1053, 550, 340, 351, 383, 376, 2143, 282, 47979, 780, 262, 3331, 1422, 470, 15771, 340, 11, 475, 428, 640, 340, 925, 340, 523, 13, 775, 651, 340, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 3181, 546, 416, 257, 1178, 36135, 25091, 11, 508, 423, 2077, 625, 6341, 290, 2839, 13598, 287, 428, 1049, 3277, 13, 198, 198, 2990, 423, 635, 5839, 510, 1181, 290, 1957, 6905, 284, 4414, 2405, 503, 286, 511, 898, 10000, 13, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 3176, 3139, 4902, 11, 262, 5287, 286, 257, 1178, 6341, 284, 2148, 12872, 5096, 319, 30401, 26, 262, 5287, 286, 262, 1230, 284, 2251, 649, 5096, 4056, 290, 2251, 649, 16538, 329, 4896, 287, 1363, 10021, 26, 290, 262, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 3298, 11754, 13, 1081, 257, 1255, 286, 262, 471, 13, 50, 13, 5627, 14310, 290, 262, 3878, 39824, 11, 867, 661, 287, 262, 471, 13, 50, 13, 389, 4137, 284, 2107, 287, 7777, 326, 389, 517, 5789, 621, 883, 287, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 995, 338, 3176, 1080, 13, 4042, 286, 514, 423, 587, 2877, 739, 257, 6279, 13, 775, 991, 2107, 739, 257, 9082, 286, 16457, 13, 383, 11754, 1080, 468, 14707, 329, 3016, 1440, 812, 13, 383, 995, 318, 287, 257, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 6381, 20168, 11, 1849, 403, 37850, 3176, 32324, 13, 554, 3269, 286, 4751, 11, 1294, 6341, 3414, 326, 484, 561, 1969, 511, 9730, 287, 968, 1971, 338, 2254, 3337, 4783, 290, 31168, 477, 7064, 287, 290, 1088, 968, 1971, 2254, 13]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "pruned_actions = []\n",
    "\n",
    "class RLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        state_size = np.prod(env.observation_space.shape)  # Total size based on the observation space shape\n",
    "        action_size = env.action_space.n\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 0.995\n",
    "        self.generated_sequences = []\n",
    "\n",
    "    def train(self, episodes=20, prompt=\"\"):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset(prompt=prompt)\n",
    "            done = False\n",
    "            action_sequence = []\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                state_index = self.convert_state_to_index(state)\n",
    "                if np.random.rand() < self.exploration_rate:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state_index])\n",
    "\n",
    "                action_sequence.append(action)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                next_state_index = self.convert_state_to_index(next_state)\n",
    "\n",
    "                self.q_table[state_index, action] = (1 - self.learning_rate) * self.q_table[state_index, action] + \\\n",
    "                                                    self.learning_rate * (reward + self.discount_factor * np.max(self.q_table[next_state_index]))\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            nextstates = state.tolist()\n",
    "            pruned_actions.extend(nextstates)\n",
    "            self.generated_sequences.append(action_sequence)\n",
    "            logging.info(f\"Episode {episode+1}/{episodes} completed with total reward: {total_reward}\")\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "\n",
    "    def convert_state_to_index(self, state):\n",
    "        if isinstance(state, list) or isinstance(state, np.ndarray):\n",
    "            flat_state = np.ravel(state)\n",
    "            index = np.dot(flat_state, np.arange(len(flat_state)))\n",
    "            return int(index) % self.q_table.shape[0]\n",
    "        return int(state) % self.q_table.shape[0]\n",
    "    def get_generated_sequences(self): \n",
    "        return self.generated_sequences\n",
    "    def print_generated_text(self):\n",
    "        print(self.env.get_last_text())\n",
    "        \n",
    "\n",
    "env = TextGenerationEnv(model, tokenizer)\n",
    "agent = RLAgent(env)\n",
    "#prompt=\"As shakespeare says in \"\n",
    "prompt = \"One of the financial scandal is the \"\n",
    "agent.train(prompt=prompt)\n",
    "\n",
    "print(f\"Pruned Actions: {pruned_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db3c2a-d016-4da7-af05-f093fed2aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/10\n",
      "One of the financial scandal is the ills of the new global banking system and the fact that those who know the financial scandals and the financial scandals have been so big that you can buy it from the New York banks when you can't get it in a bank.\n",
      "\n",
      "And the great\n",
      "Iteration: 2/10\n",
      "One of the financial scandal is the ills of the financial world. The problem with financial capitalism is that it is not the working class who has to be the financial workers, but the working class. In the banking economy, it is not the banks who create money. In this crisis,\n",
      "Iteration: 3/10\n",
      "One of the financial scandal is the  massive massive in-accountancy system in which a bank's accounts and accounts-\n",
      "accounts- are bought or \"dis-\n",
      "\n",
      "dis-\n",
      "\n",
      "salered accounts. The  massive\n",
      "Iteration: 4/10\n",
      "One of the financial scandal is the ills of the \"bank.\n",
      "\n",
      "Fol, we can't let the other world's banks get what they've been forced to give us. We're in the business of trying to get people to actually give us more money, and we're\n",
      "Iteration: 5/10\n",
      "One of the financial scandal is the ills of the financial system, which are still being made up in the political and financial world. We are going to provide an option for people to make the right financial investment, by making a money back to the government through banks, in many private banks\n",
      "Iteration: 6/10\n",
      "One of the financial scandal is the ills of the bankers and the government on which the political and financial system, and the government in the rest of the world, have been going over. The bankers are right and the government has been right. The financial crisis has been a political failure and\n",
      "Iteration: 7/10\n",
      "One of the financial scandal is the ills of the system that have made up such a big money problem for the financial system, and the problem is a great one.\n",
      "\n",
      "One of the money problem is that of \"public goods\" or \"public goods that are not private, but\n",
      "One of the financial scandal is the urchin, which was forced to buy the firm and was, of necessity, used to make loans.\n",
      "\n",
      "\n",
      "The urchin has a big name to its name and it's making that money from this kind of business. The bank has a\n",
      "Iteration: 8/10\n",
      "One of the financial scandal is the ills of the \"Lol\" that are being used by the state to create and fund a crime.\n",
      "\n",
      "It's the big problem, and the big problem, being that the nation is being led by big money for the benefit of big banks\n",
      "Iteration: 9/10\n",
      "One of the financial scandal is the vernacular of being \"the money-man of the world\" by being the \"bank of money\" by the financial system.\n",
      "\n",
      "And so it is that the world is a new world, a new global banking system in which the financial system\n",
      "Iteration: 10/10\n",
      "One of the financial scandal is the  S&P and New York's housing bubble and has led to a new investment bubble.\n",
      "The government and private equity funds have to be held up for a long time, as they are the one big regulator on housing that is only working to\n",
      "One of the financial scandal is the  disastrous the \"Financial and a New York City\" bailout that has been made by \"sens. of the people\" and which has led to a new class of people who, after being \"the \"disastrous\"  s\n",
      "One of the financial scandal is the ills and the great recession. So, what you can't and how we can get to the state of financial crisis is an economy that is in a great recession right now.\n",
      "\n",
      "The other big issue was about the economy, and we're in\n",
      "One of the financial scandal is the ills of its own financial system, and this crisis is not one that is going to be brought about by those who are in a financial crisis. I also have the sense that I have been so \"al-the-unmanner\" as to\n",
      "One of the financial scandal is the ills it has brought on themselves.\n",
      "\n",
      "The people who have made the issue a political issue are many.\n",
      "\n",
      "They can be many.\n",
      "\n",
      "The money used to buy and buy.\n",
      "\n",
      "The money used to make the world a more\n",
      "One of the financial scandal is the  disastrous failure of \"the government to disclose its illegal and illegal bank loans to the big banks\". \n",
      "And this is in a time of crisis of the financial system that is, of the financial system that has been under \"the government's\n",
      "Pruned: Total Time: 41.16968083381653; Average Time 4.116968083381653\n",
      "One of the financial scandal is the  disastrous failure of \"the government to disclose its illegal and illegal bank loans to the big banks\". \n",
      "And this is in a time of crisis of the financial system that is, of the financial system that has been under \"the government's\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "pruned_action_space = pruned_actions\n",
    "\n",
    "def top_k_top_p_filtering( logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    assert logits.dim() == 2  # logits should be [batch_size, vocab_size]\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "        logits[0,indices_to_remove] = filter_value\n",
    "    return logits\n",
    "    \n",
    "def generate_custom_text_pruned( inputs,input_ids, max_length=50, top_k=50, top_p=0.95):\n",
    "\n",
    "    # Initialize generated tokens list\n",
    "    done = False\n",
    "    while not done:\n",
    "        generated_text = \"\"\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids=generated)\n",
    "        \n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Limit vocab by lowering the probailities for all other tokens except pruned\n",
    "            mask = torch.full(next_token_logits.shape, float('-inf'))\n",
    "            \n",
    "            for token_id in pruned_action_space:\n",
    "                mask[:, token_id] = next_token_logits[:, token_id]\n",
    "            next_token_logits = mask\n",
    "           \n",
    "            # Sampling\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "    \n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop generating if the end-of-sequence token is generated\n",
    "            if next_token in tokenizer.encode(tokenizer.eos_token):\n",
    "                break\n",
    "    \n",
    "        generated_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "        print(generated_text)\n",
    "        if \"bank\" in generated_text or \"Bank\" in generated_text:\n",
    "            done = True\n",
    "            \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "\n",
    "tot_time = 0   \n",
    "start_time = time.time()\n",
    "for ind in range(50):\n",
    "    print(f\"Iteration: {ind+1}/50\") \n",
    "    generated_text = generate_custom_text_pruned(inputs, input_ids)\n",
    "end_time = time.time()\n",
    "tot_time += (end_time - start_time)\n",
    "avg_time = tot_time/50\n",
    "print(f\"Pruned: Total Time: {tot_time}; Average Time {avg_time}\")\n",
    "\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c370bb-edd1-4134-b235-53d1f52aaf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/10\n",
      "One of the financial scandal is the ersatz money transfers that led to the massive rescue of Japan from the Nazis during World War II. Some of this money flows directly to the US government, such as JP Morgan Chase. JP Morgan Chase has been the recipient of over 1 billion dollars of\n",
      "One of the financial scandal is the  unsuccessful and failed attempt by Paul Ryan to keep the House Republicans from passing a tax cut for middle-class Americans last month.  According to Bloomberg , the tax bill's increase in the deficit should have helped keep the House\n",
      "One of the financial scandal is the ills of the financial system that has plagued the United States for the past several years. What can we do to help? We can do both. If we can create a global and growing financial system, including a robust national and regional system, we can\n",
      "One of the financial scandal is the ills of the 'investor's state.' We could go to the source of the scandal, or we could get the story by an independent journalist or we could just ignore the source, but at least it might give you a picture of what is going\n",
      "One of the financial scandal is the izakaya. It can have many meanings,\" said Mr. Karashenka, an accountant. And for many the scandal is what attracts financial transactions and brings them in as well. \"The best deal on the street is the izak\n",
      "One of the financial scandal is the  investment and the 'investment' in the state and the state's ability to influence people, with which the oligarchs have become so close to each other's children, in order to profit for the oligarchs, who have always been\n",
      "One of the financial scandal is the icky details about the bank that manages UBS's loans and mortgages. If you're lucky, you might be able to find a new account in the UBS banking group. If not, the UBS Group Account will provide you with an email address\n",
      "Iteration: 2/10\n",
      "One of the financial scandal is the Ṣakaṭhūnāsa, which was an innovation in ancient India. It could be seen during the reign of V. J. T. Vardhan who was the Chief Minister of Uttar Pradesh from 1835-1939\n",
      "One of the financial scandal is the ikki. For the most part, many are unaware of the existence of the ikki and the ikki-monsters. Yet the ikki is known to be an indispensable tool for the ikki-monsters.\n",
      "\n",
      "One of the financial scandal is the  bizarre investment and subsequent accident with Mr. Cunha who died in 2001.\n",
      "It was a  accident that exposed a deep-rooted insufferable conspiracy to cover up the\n",
      "One of the financial scandal is the ills of government corruption, the erosion of its power, its control over the budget and the use of special tax advantages for the rich. Yet a number of such corruption scandals have been reported since this time last year — the most recent of which involves the\n",
      "One of the financial scandal is the ersatz, massive, multibillion-dollar scheme whereby a single individual or company of any size, including one or more major bank-affiliated entities, can influence another person's position, including for the purposes of influencing an election, vote, or\n",
      "Iteration: 3/10\n",
      "One of the financial scandal is the ills of the U.S., where banks that had been run by the Bush White House were being bought out by the Bush-appointed Treasury Department.\n",
      "\n",
      "The House Energy and Commerce Committee has already subpoenaed former Treasury Department director Hank Paulson and\n",
      "Iteration: 4/10\n",
      "One of the financial scandal is the ersatz investment in Chinese financial firms in Asia, one of the largest and most prestigious in the world, which has attracted large investments and is the major component of global growth. But even though China's economy has expanded a bit, the Chinese have never\n",
      "One of the financial scandal is the  financial crisis. In 2008, the government announced it would limit its purchases of more than $1 trillion in bonds, bonds designed to prop up the financial system and provide bailouts to the banks. By 2009, only one company in 25 American corporations\n",
      "Iteration: 5/10\n",
      "One of the financial scandal is the  investments that have been made in and under the Bank of England and other state and local authorities.\n",
      "A spokesman for the bank confirmed that its decision to pull the plug on all public services was \"the correct one\".\n",
      "\"This is a\n",
      "Iteration: 6/10\n",
      "One of the financial scandal is the  profit of many high-profile government corruption cases, including the US$2.2 trillion US prison scandal which forced more than 7,000 prisoners to forfeit more than US$1 billion from their bank accounts. In February 2011, four prosecutors convicted\n",
      "Iteration: 7/10\n",
      "One of the financial scandal is the  possible involvement of three former officers: the  Director of National Intelligence, James Clapper, former  CIA Director  Cheryl Ann Rice and CIA Director Mike Pompeo of Russia .\n",
      "In May,\n",
      "One of the financial scandal is the ute company, a small-group of well-connected men with connections to big banks, lobbyists, lobbyists for big government, and hedge fund managers. It was a scandal, and yet it's one of the most scandalous scandals of recent history.\n",
      "Iteration: 8/10\n",
      "One of the financial scandal is the ills that can be created within this country. These are not the things that we are talking about, these are the things that can be done. When you get a government that you trust to do the right thing, then you can make sure it doesn\n",
      "One of the financial scandal is the ills of the \"death of Satoshi Nakamoto.\" One aspect that would likely change dramatically would be the role that the Bitcoin Foundation plays. It would mean that the Bitcoin Foundation would be given more freedom, and that's not a bad thing. But it\n",
      "One of the financial scandal is the icky nature of the story.\n",
      "\n",
      "\"The person making the announcement today was not aware of any changes in terms of the type of transaction and how many of the shares were given to the customer. There may be some exceptions to that rule.\"\n",
      "\n",
      "One of the financial scandal is the ills of the system—too many large firms, too many people, too many banks.\n",
      "\n",
      "The real crime of the crisis is that this country's economic growth is stagnant, and that too many people suffer from it. That it's a crisis\n",
      "Iteration: 9/10\n",
      "One of the financial scandal is the ills of the financial system. But it's not just financial issues; it's the issues that impact the lives of the millions of people who work in the middle class in America.\n",
      "One of the financial scandal is the ills of our government, the government that's committed to keeping your people safe and getting them out of poverty. These are real issues. They're not small potatoes that are part of a larger problem. I think this is really one of the problems.\"\n",
      "One of the financial scandal is the ills of corporate capitalism.\n",
      "\n",
      "And that's something that's been proven time and time again. The only reason we have problems in the finance industry is because of the ways things are done. We live in a free market. You know where everything\n",
      "One of the financial scandal is the ills caused by Wall Street. Over the past couple of years, we've seen the worst and fastest growth in US retail and consumer spending. We're not going to forget that as the economy has gotten better, more and more corporations and individuals have gone\n",
      "One of the financial scandal is the ursant's personal account that was discovered during the investigation, which is also known as the \"Panama Papers.\"\n",
      "\n",
      "This may also explain why the Financial Times reported in January that some of their own emails may be in the hands of Russia's\n",
      "One of the financial scandal is the ills of financial speculation which are often linked to an underlying problem of trust. Financial scandals can also be a catalyst for a more political, populist and authoritarian form of governance. At the same time, there are also growing fears that the economic policies of most\n",
      "One of the financial scandal is the  of the ''Bank of New York'' as we have seen in relation to the manipulation of the market.  The financial crisis was caused by a crisis of capital that was designed to cause a loss to the credit ''givers of money\n",
      "Iteration: 10/10\n",
      "One of the financial scandal is the ills that have swept the world since 2005 - the global financial meltdown, Wall Street scandals - but the global financial markets have been so safe that even when a U.S. mortgage lender gets its loan approved, no investor in that country should have to\n",
      "One of the financial scandal is the ills associated with the sale of the 'Bible', the Christian version of the Bible that is the foundation of all religious belief and practice.\n",
      "\n",
      "\n",
      "In fact, since the start of the 'bible', a lot of information about biblical teaching is\n",
      "One of the financial scandal is the ills it has wrought for the local economy.\n",
      "\n",
      "The only economic recovery since 1990 has come from lower consumer prices. At best, that's not an impressive feat. A typical American household of $25,000 may have seen 10 percent or more\n",
      "One of the financial scandal is the ikad. It's money that goes to the king's official personal fund. The royal family funds their expenses in real estate, such as their estate tax bill, housekeeping or catering. In the real estate transaction, this is the source of their\n",
      "One of the financial scandal is the vernacular of the financial crisis, as people are sold their wealth and become rich. If you like the term 'bail-ins,' you better believe that is the default of the big banks, who are responsible for billions in losses. That's\n",
      "Not Pruned: Total Time: 77.04904699325562; Average Time 7.704904699325562\n",
      "One of the financial scandal is the vernacular of the financial crisis, as people are sold their wealth and become rich. If you like the term 'bail-ins,' you better believe that is the default of the big banks, who are responsible for billions in losses. That's\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def top_k_top_p_filtering( logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    assert logits.dim() == 2  # logits should be [batch_size, vocab_size]\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "        logits[0,indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def generate_custom_text( inputs,input_ids, max_length=50, top_k=50, top_p=0.95):\n",
    "\n",
    "    # Initialize generated tokens list\n",
    "    done = False\n",
    "    while not done:\n",
    "        generated_text = \"\"\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids=generated)\n",
    "    \n",
    "    \n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Mask tokens not in custom action space by setting their logits to a very low value\n",
    "            #mask = torch.full(next_token_logits.shape, float('-inf'))\n",
    "            \n",
    "            #for token_id in pruned_action_space:\n",
    "            #    mask[:, token_id] = next_token_logits[:, token_id]\n",
    "            #next_token_logits = mask\n",
    "           \n",
    "            # Apply sampling techniques\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "    \n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop generating if the end-of-sequence token is generated\n",
    "            if next_token in tokenizer.encode(tokenizer.eos_token):\n",
    "                break\n",
    "\n",
    "        generated_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "        print(generated_text)\n",
    "        if \"bank\" in generated_text or \"Bank\" in generated_text:\n",
    "            done = True\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "tot_time = 0   \n",
    "start_time = time.time()\n",
    "for ind in range(50):\n",
    "    print(f\"Iteration: {ind+1}/50\") \n",
    "    generated_text = generate_custom_text(inputs, input_ids)\n",
    "end_time = time.time()\n",
    "tot_time += (end_time - start_time)\n",
    "avg_time = tot_time/50\n",
    "print(f\"Not Pruned: Total Time: {tot_time}; Average Time {avg_time}\")\n",
    "\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b8264-3e86-43b3-8658-c4cbf0bb7d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910e9ca-2b6d-4e5e-ba9c-95349a983c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
